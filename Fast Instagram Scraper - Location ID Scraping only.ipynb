{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Fast Instagram Scraper v1.0.0\n",
    "# Release: 22.11.2020\n",
    "# GitHub: do-me\n",
    "#################################\n",
    "\n",
    "# install torpy and pandas before\n",
    "\n",
    "from torpy.http.requests import TorRequests\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# scraping parameters\n",
    "\n",
    "# main\n",
    "location_id = 123456 \n",
    "max_posts = 100000000 # maximum number of posts to scrape\n",
    "out_dir = \"/some/path/\" # directory to save csv file\n",
    "\n",
    "# advanced\n",
    "max_requests = 10000 # maximum number of requests from one tor end node\n",
    "wait_between_requests = 0 # time in seconds to wait for next requests adding up to normal execution time ~8 seconds\n",
    "max_tor_renew = 10000 # maximum number of new tor sessions\n",
    "run_number = \"\" # will be added to filename; useful for pausing and resuming, see comment in next cells\n",
    "\n",
    "# just in the beginning: define empty variables\n",
    "# IMPORTANT: when pausing (=interrupting jupyter) and resuming do not execute this cell! \n",
    "# just execute the main loop below as last_cursor and post_list will be in memory\n",
    "post_list = []\n",
    "\n",
    "# a cursor is an arbitrary hash to paginate through Instagram's posts\n",
    "# when opening the first page with 50 results it contains a hash to load the next page and so on\n",
    "# it's pretty much like a page number just that the numbers are not in order \n",
    "# for the first page there is no cursor, so leave it empty\n",
    "last_cursor = \"\" \n",
    "this_cursor = \"\"\n",
    "\n",
    "# help function\n",
    "# returns right Instagram link\n",
    "def ilink(this_locid = location_id, cursor=\"\"):\n",
    "    instalink = 'https://instagram.com/graphql/query/?query_hash=ac38b90f0f3981c42092016a37c59bf7&variables={\"id\":\"' + str(this_locid) + '\",\"first\":50,\"after\":\"'+ cursor +'\"}'\n",
    "    return instalink\n",
    "\n",
    "total_posts = 0\n",
    "# main scraping function\n",
    "def torsession():\n",
    "    global last_cursor, this_cursor, location_id, post_list, out_dir, run_number, total_posts\n",
    "    \n",
    "    with TorRequests() as tor_requests:\n",
    "        with tor_requests.get_session() as sess, tqdm(total=2000) as pbar:\n",
    "            print(\"Circuit built.\") # conncection works\n",
    "            i = 0\n",
    "            \n",
    "            while i < max_requests: # enter main loop\n",
    "                print(\"Start iteration {}: {}\".format(i,datetime.datetime.now()))\n",
    "\n",
    "                # saves all posts as csv for every iteration, 50 at once\n",
    "                pf = json_normalize(post_list)\n",
    "                \n",
    "                file_name = \"{}{}{}.csv\".format(out_dir, location_id, run_number)\n",
    "                pf.to_csv(file_name, index=False)\n",
    "                print(\"File saved.\")\n",
    "            \n",
    "                time.sleep(wait_between_requests) # take a nap\n",
    "                              \n",
    "                try: \n",
    "                    ireq = sess.get(ilink(cursor = last_cursor)) # fire request\n",
    "                    idata = ireq.json() # get data from page as json\n",
    "                    \n",
    "                except:\n",
    "                    try:\n",
    "                        print(\"Tor end node blocked. Last response: {}\".format(ireq))\n",
    "                    except:\n",
    "                        print(\"Tor end node blocked.\")\n",
    "                    return # go back to main loop and get next session\n",
    "                \n",
    "                # access response json\n",
    "                edge_location_to_media = idata[\"data\"][\"location\"][\"edge_location_to_media\"]\n",
    "                \n",
    "                # if while scraping new posts appear, they will be considered!\n",
    "                total_posts = edge_location_to_media[\"count\"]\n",
    "                pbar.total = total_posts\n",
    "                pbar.refresh()\n",
    "                \n",
    "                ipage = edge_location_to_media[\"edges\"] # get posts\n",
    "                post_list.extend(ipage) # extend list with all posts (50 every time)\n",
    "                pbar.update(len(ipage))\n",
    "                print(\"Succesfully appended iteration: {}\".format(i))\n",
    "                \n",
    "                if len(post_list) > max_posts:\n",
    "                    print(\"Maximum number of posts scraped:{}\".format(len(post_list)))\n",
    "                    return \"no_more_page\"\n",
    "                \n",
    "                # return completely if no more page available (has_next_page: False)\n",
    "                if not edge_location_to_media[\"page_info\"][\"has_next_page\"]:\n",
    "                    return \"no_more_page\"\n",
    "                \n",
    "                # just in case above doesn't work: compare this and last cursor\n",
    "                this_cursor = edge_location_to_media[\"page_info\"][\"end_cursor\"]\n",
    "                if this_cursor == last_cursor:\n",
    "                    return \"no_more_page\"\n",
    "\n",
    "                last_cursor = this_cursor  \n",
    "                \n",
    "                # for long pause and jupyter shutdown: saves only the last cursor\n",
    "                open(\"{}{}_last_cursor.txt\".format(out_dir,location_id), 'a').write(last_cursor+\"\\n\")\n",
    "                # alternatively just print last_cursor for every iteration\n",
    "                # print(last_cursor)\n",
    "                \n",
    "                i+=1   \n",
    "\n",
    "# main loop\n",
    "def scrape():\n",
    "    ii = 0 \n",
    "    while ii < max_tor_renew:\n",
    "        print(\"Initiating tor session {}\".format(ii))\n",
    "        if torsession() == \"no_more_page\": \n",
    "            print(\"Mined {} from {} total posts.\".format(len(post_list),total_posts))\n",
    "            break\n",
    "        ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape! \n",
    "scrape()\n",
    "\n",
    "# Short Breaks\n",
    "# for pausing just interrupt the jupyter notebook and after to resume only re-execute this cell \n",
    "# all variables will still be in memory and the script takes off where it left\n",
    "# works only if jupyter server is not closed meanwhile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Breaks\n",
    "# just interrupt and close everything when jupyter or computer is shut down\n",
    "# afterwards execute first cell (to reload all functions) and then this cell\n",
    "# it will get the last cursor and add run_number to filename\n",
    "\n",
    "# uncomment the following lines\n",
    "# from pathlib import Path\n",
    "# last_cursor = Path(\"{}{}_last_cursor.txt\".format(out_dir,location_id)).read_text().split(\"\\n\")[-2] # reads last cursor\n",
    "# run_number = int(datetime.datetime.now().timestamp()) # change to some index number or just leave the timestamp but watch out for duplicates!\n",
    "# scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
