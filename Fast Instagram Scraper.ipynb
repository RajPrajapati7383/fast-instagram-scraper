{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Instagram Scraper v1.2.1 \n",
    "## Release: 22.11.2020\n",
    "## GitHub: https://github.com/do-me/fast-instagram-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use \n",
    "\n",
    "0. Make sure to have all neccessary packages installed (see imports from cell 1)\n",
    "1. Execute cell 1 below to load functions\n",
    "2. a) For a single hashtag or location id to scrape, adjust parameters in cell 2 and execute <br>\n",
    "   b) For multiple hashtags and/or location ids adjust params in cell 3 and execute \n",
    "3. For short breaks see cell 4, for long breaks see cell 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\n",
    "# Quickstart: leave defaults in this cell, execute and go to cell 2\n",
    "\n",
    "# install torpy, tqdm and pandas before\n",
    "from torpy.http.requests import TorRequests\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm # progress bar\n",
    "from IPython.display import display, Markdown # for nicer printing\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "\n",
    "tor_timeout = 600\n",
    "# just in the beginning: define empty variables\n",
    "# IMPORTANT: when pausing (=interrupting jupyter) and resuming do not execute this cell! \n",
    "# just execute the main loop below as last_cursor and post_list will be in memory\n",
    "post_list = []\n",
    "\n",
    "# a cursor is an arbitrary hash to paginate through Instagram's posts\n",
    "# when opening the first page with 50 results it contains a hash to load the next page and so on\n",
    "# it's pretty much like a page number just that the numbers are not in order \n",
    "# for the first page there is no cursor, so leave it empty\n",
    "last_cursor = \"\" \n",
    "this_cursor = \"\"\n",
    "\n",
    "### Instagram hashes\n",
    "location_hash = \"ac38b90f0f3981c42092016a37c59bf7\" # might change in the future\n",
    "hashtag_hash = \"ded47faa9a1aaded10161a2ff32abb6b\" # borrowed from https://github.com/arc298/instagram-scraper/blob/master/instagram_scraper/constants.py\n",
    "\n",
    "# help function\n",
    "# returns right Instagram link\n",
    "def ilink(cursor=\"\"):\n",
    "    if location_or_hashtag == \"location\":\n",
    "        instalink = 'https://instagram.com/graphql/query/?query_hash='+location_hash+'&variables={\"id\":\"' + str(object_id_or_string) + '\",\"first\":50,\"after\":\"'+ cursor +'\"}'\n",
    "        return instalink\n",
    "    elif location_or_hashtag == \"hashtag\":\n",
    "        instalink = 'https://instagram.com/graphql/query/?query_hash='+hashtag_hash+'&variables={\"tag_name\":\"' + str(object_id_or_string) + '\",\"first\":50,\"after\":\"'+ cursor +'\"}'\n",
    "        return instalink\n",
    "    else:\n",
    "        raise RuntimeError('location_or_hashtag variable must be location or hashtag')        \n",
    "\n",
    "# define keys to delete here \n",
    "def delete_keys(f_node):\n",
    "    if 'thumbnail_resources' in f_node: del f_node['thumbnail_resources']\n",
    "    if 'thumbnail_src' in f_node: del f_node['thumbnail_src']\n",
    "    return f_node\n",
    "\n",
    "# adds (redundant) location information to every post\n",
    "def add_location_data(l_node):\n",
    "    l_node[\"location_id\"] = ploc[\"id\"]\n",
    "    l_node[\"location_name\"] = ploc[\"name\"]\n",
    "    l_node[\"location_slug\"] = ploc[\"slug\"]\n",
    "    l_node[\"location_latlong\"] = [ploc[\"lat\"],ploc[\"lng\"]]\n",
    "    return l_node\n",
    "\n",
    "# executes above functions for a list of posts (JSON-nodes)\n",
    "def add_locations_data_to_cleaned_node(nodelist, just_clean = False):\n",
    "    if just_clean == True:\n",
    "        nodelist = [delete_keys(i[\"node\"]) for i in nodelist]\n",
    "        return nodelist\n",
    "    else:\n",
    "        nodelist = [add_location_data(delete_keys(i[\"node\"])) for i in nodelist] # chained functions and list comprehension\n",
    "        return nodelist\n",
    "\n",
    "total_posts = 0\n",
    "ploc = None\n",
    "# main scraping function\n",
    "def torsession(first_iter = False):\n",
    "    global last_cursor, this_cursor, post_list, run_number, total_posts, ploc\n",
    "    \n",
    "    with TorRequests() as tor_requests:\n",
    "        with tor_requests.get_session() as sess, tqdm(total=0) as pbar:\n",
    "            print(\"Circuit built.\") # conncection works\n",
    "            i = 0\n",
    "            \n",
    "            while i < max_requests: # enter main loop\n",
    "                print(\"Start iteration {}: {}\".format(i,datetime.datetime.now()))\n",
    "\n",
    "                # saves all posts as csv for every iteration, 50 at once\n",
    "                pf = pd.json_normalize(post_list)\n",
    "                \n",
    "                file_name = \"{}{}{}.csv\".format(out_dir, object_id_or_string, run_number)\n",
    "                pf.to_csv(file_name, index=False)\n",
    "                print(\"File saved.\")\n",
    "            \n",
    "                time.sleep(wait_between_requests) # take a nap\n",
    "                              \n",
    "                try: \n",
    "                    ireq = sess.get(ilink(cursor = last_cursor)) # fire request\n",
    "                    idata = ireq.json() # get data from page as json\n",
    "                    \n",
    "                except:\n",
    "                    try:\n",
    "                        print(\"Tor end node blocked. Last response: {}\".format(ireq))\n",
    "                    except:\n",
    "                        print(\"Tor end node blocked.\")\n",
    "                    return # go back to main loop and get next session\n",
    "                \n",
    "                if idata[\"data\"][location_or_hashtag] == None:\n",
    "                    print(\"No posts available!\")\n",
    "                    return \"no_more_page\"\n",
    "                    \n",
    "                # access response json\n",
    "                edge_to_media = idata[\"data\"][location_or_hashtag][\"edge_{}_to_media\".format(location_or_hashtag)]\n",
    "                \n",
    "                # if while scraping new posts appear, they will be considered!\n",
    "                total_posts = edge_to_media[\"count\"]\n",
    "                pbar.total = total_posts\n",
    "                pbar.refresh()\n",
    "                \n",
    "                ipage = edge_to_media[\"edges\"] # get posts\n",
    "                \n",
    "                # append location information for location scraping\n",
    "                if location_or_hashtag == \"location\":\n",
    "                    ploc = idata[\"data\"][location_or_hashtag]\n",
    "                    ipage = add_locations_data_to_cleaned_node(ipage)\n",
    "                else: \n",
    "                    ipage = add_locations_data_to_cleaned_node(ipage, just_clean=True)\n",
    "                \n",
    "                post_list.extend(ipage) # extend list with all posts (50 every time)\n",
    "                pbar.update(len(ipage))\n",
    "                \n",
    "                print(\"Succesfully appended iteration: {}\".format(i))\n",
    "                \n",
    "                if len(post_list) > max_posts:\n",
    "                    print(\"Maximum number of posts scraped:{}\".format(len(post_list)))\n",
    "                    return \"no_more_page\"\n",
    "                \n",
    "                # return completely if no more page available (has_next_page: False)\n",
    "                if not edge_to_media[\"page_info\"][\"has_next_page\"]:\n",
    "                    if len(post_list) < max_posts:\n",
    "                        print(\"Maybe you scraped too fast. Try setting a higher wait_between_requests\")\n",
    "                        return \"no_more_page\"\n",
    "                    else:\n",
    "                        return \"no_more_page\"\n",
    "                \n",
    "                # just in case above doesn't work: compare this and last cursor\n",
    "                this_cursor = edge_to_media[\"page_info\"][\"end_cursor\"]\n",
    "                if this_cursor == last_cursor:\n",
    "                    return \"no_more_page\"\n",
    "\n",
    "                last_cursor = this_cursor  \n",
    "                \n",
    "                # for long pause and jupyter shutdown: saves only the last cursor\n",
    "                open(\"{}{}_last_cursor.txt\".format(out_dir,object_id_or_string), 'a').write(last_cursor+\"\\n\")\n",
    "                # alternatively just print last_cursor for every iteration\n",
    "                # print(last_cursor)\n",
    "                \n",
    "                i+=1   \n",
    "\n",
    "# main loop\n",
    "def scrape():\n",
    "    ii = 0 \n",
    "    while ii < max_tor_renew:\n",
    "        print(\"Initiating tor session {}\".format(ii))\n",
    "        \n",
    "        # timeout try/except with https://github.com/kata198/func_timeout\n",
    "        try:\n",
    "            if func_timeout(tor_timeout, torsession) == \"no_more_page\": \n",
    "                print(\"Mined {} from {} total posts.\".format(len(post_list),total_posts))\n",
    "                break\n",
    "        except FunctionTimedOut:\n",
    "            print (\"Torsession terminated after {} seconds tor_timeout.\".format(tor_timeout))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Scrape a single hashtag or location id \n",
    "\n",
    "# main parameters \n",
    "location_or_hashtag = \"hashtag\" # location or hashtag\n",
    "object_id_or_string = \"truckfonalddump\" # a string for hashtag an int for location id i.e. 12345678\n",
    "max_posts = 10000 # maximum number of posts to scrape\n",
    "out_dir = \"/path/to/dir\" # directory to save csv file\n",
    "\n",
    "# advanced parameters\n",
    "max_requests = 10000 # maximum number of requests from one tor end node\n",
    "wait_between_requests = 5 # time in seconds to wait for next requests adding up to normal execution time ~ 4-8 seconds\n",
    "max_tor_renew = 10000 # maximum number of new tor sessions\n",
    "run_number = \"\" # will be added to filename; useful for pausing and resuming, see comment in cell5\n",
    "\n",
    "# scrape!\n",
    "scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scrape multiple hashtags and/or location ids\n",
    "\n",
    "# technically just iterating through scrape_items_list and executes scrape() for every item with new variables\n",
    "# it is possible to scrape for both location ids and hashtags at the same time \n",
    "# by passing \"hashtag\" or \"location\" in location_or_hashtag_list\n",
    "\n",
    "# main parameters \n",
    "scrape_items_list = [\"justdoit\",\"truckfonalddump\"]\n",
    "location_or_hashtag = \"hashtag\" # location or hashtag, will be overwritten when location_or_hashtag_list is passed\n",
    "max_posts = 10000 # maximum number of posts to scrape\n",
    "out_dir = \"/path/to/dir/\" # directory to save csv file\n",
    "\n",
    "# scraping for heterogenous values (locatoin and hashtags) use as below\n",
    "# scrape_items_list = [\"12345678\",\"justdoit\"]\n",
    "# location_or_hashtag_list = [\"location\",\"hashtag\"]\n",
    "\n",
    "# advanced parameters\n",
    "max_requests = 10000 # maximum number of requests from one tor end node\n",
    "wait_between_requests = 5 # time in seconds to wait for next requests adding up to normal execution time ~ 4-8 seconds\n",
    "max_tor_renew = 10000 # maximum number of new tor sessions\n",
    "run_number = \"\" # will be added to filename; useful for pausing and resuming, see comment in cell5\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# main loop for scrape_items_list\n",
    "for index, element in enumerate(tqdm(scrape_items_list)):\n",
    "    object_id_or_string = element\n",
    "    \n",
    "    ### start: only relevant for heterogenous values \n",
    "    if \"location_or_hashtag_list\" in globals():\n",
    "        if len(location_or_hashtag_list) == len(scrape_items_list):\n",
    "            location_or_hashtag = location_or_hashtag_list[index]\n",
    "        if len(location_or_hashtag_list) != len(scrape_items_list) and index == 0:\n",
    "            print('location_or_hashtag_list must have same length as scrape_items_list\\nScraping {} for every item as defined globally.'.format(location_or_hashtag))  \n",
    "    ### end: only relevant for heterogenous values \n",
    "\n",
    "    display(Markdown('# {}. Mining for {}:{}'.format(index, location_or_hashtag, object_id_or_string)))\n",
    "    last_cursor = this_cursor = \"\" # reset cursors\n",
    "    post_list = []\n",
    "    scrape()\n",
    "    \n",
    "    # use this try except block when scraping over a longer period so that any ocurring error doesn't stop the main loop\n",
    "    # try:\n",
    "    #     scrape()\n",
    "    # except:\n",
    "    #     print(\"Finished with error - see log. Continuing with next item.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Short Breaks\n",
    "\n",
    "# for pausing just interrupt the jupyter notebook and after to resume only re-execute this cell \n",
    "# all variables will still be in memory and the script takes off where it left\n",
    "# works only if jupyter server is not closed meanwhile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Long Breaks\n",
    "\n",
    "# just interrupt and close everything when jupyter or computer is shut down\n",
    "# afterwards execute first cell (to reload all functions) and then this cell\n",
    "# it will get the last cursor and add run_number to filename\n",
    "\n",
    "from pathlib import Path\n",
    "last_cursor = Path(\"{}{}_last_cursor.txt\".format(out_dir,object_id_or_string)).read_text().split(\"\\n\")[-2] # reads last cursor\n",
    "run_number = int(datetime.datetime.now().timestamp()) # change to some index number or just leave the timestamp but watch out for duplicates!\n",
    "scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
